# this is to get familiar with pyTorch. E.g. Dataloader, nn and conv architecture in general
# This is to achieve an autoencoder for images already as numpy PIL array (generated by ImgStandardizer.py)
# focus only on these E:\myAWS\photographSelection_real\standard\real_std_1024x768
import numpy as np
import torch
from torch.utils.data import Dataset,DataLoader
import torchvision.transforms as transforms
import torch.optim as optim
from PIL import Image
import netarchitecture1
import torch.nn as nn
from torch.autograd import Variable

device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
torch.set_num_threads(6)
EPOCH_SIZE = 4

class MyDataset(Dataset):
    def __init__(self, data,  transform=None):
        self.data = data
        #self.targets = torch.LongTensor(targets)
        self.transform = transform
        print(str(data.shape))
    def __getitem__(self, index):
        x = self.data[index]
        if self.transform:
            x = Image.fromarray(self.data[index].astype(np.uint8).transpose(0,1,2))
            x = self.transform(x)
        return x
    def __len__(self):
        return len(self.data)


def load_dataset( Path, IMAGE_SIZE , BATCH_SIZE):
    dat = np.load(Path)
    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])
    dataset = MyDataset(dat, transform=transform)
    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, drop_last=True, shuffle=True)
    return dataloader


def train(dataloader, IMAGE_SIZE , BATCH_SIZE):
    max_data = len(dataloader)
    start_epoch = 0
    epochs = np.array([i for i in range(start_epoch, EPOCH_SIZE)], dtype=np.uint8)
    print("Epoch Size:",EPOCH_SIZE)
    net_generator = network.Generator().apply(network.weights_init)
    net_discriminator = network.Discriminator().apply(network.weights_init)
    print(net_generator.parameters())
    optimizer_generator = optim.Adam(net_generator.parameters(),
                                     lr=2e-4,
                                     betas=(.5, .999))

    optimizer_discriminator = optim.Adam(net_discriminator.parameters(),
                                         lr=2e-4,
                                         betas=(.5, .999))
    criterion = nn.BCELoss()
    max_data = len(dataloader)
    epochs = np.array(
        [i for i in range(start_epoch, EPOCH_SIZE)], dtype=np.uint8)

    for epoch in np.nditer(epochs):
        for idx, data in enumerate(dataloader, 0):
            print("idx: " + str(idx) + ", shape of data: " + str(data.shape))

            net_discriminator.zero_grad()
            ## train discriminiator on real images
            real_img = Variable(data)  # old version took only first element of batch data[0]
            batch_size = real_img.size()[0]
            print("real image dim" ,real_img.shape)
            ones = Variable(torch.ones(batch_size))
            output = net_discriminator.forward(real_img)
            print("ONES",ones.size())
            real_img_error = criterion(output, ones)
            print("ffdehsh")

            ## train discriminiator on fake images
            noise_tensor = Variable(torch.randn(batch_size, 100, 1, 1))
            print("ffdehsh")
            fake_img = net_generator.forward(noise_tensor)
            zeros = Variable(torch.zeros(batch_size))
            print(zeros.shape)
            # detach the gradient from generator (saves computation)
            print("fake image dim",fake_img.shape)
            output = net_discriminator.forward(fake_img.detach())
            fake_img_error = criterion(output, zeros)

            ## backpropagate total error
            descriminator_error = real_img_error + fake_img_error
            descriminator_error.backward()
            optimizer_discriminator.step()

            ## 2. update weights of generator
            net_generator.zero_grad()
            # now we keep the gradient so we can update the weights of the generator
            output = net_discriminator.forward(fake_img)
            generator_error = criterion(output, ones)
            generator_error.backward()
            optimizer_generator.step()





if __name__ == '__main__':
    DATALOADER = load_dataset(Path="E:\myAWS\photographSelection_real\standard\\real_std_800x600\\train1.npy", IMAGE_SIZE=(800,600), BATCH_SIZE=2)
    train(DATALOADER, IMAGE_SIZE=(800,600), BATCH_SIZE=2)